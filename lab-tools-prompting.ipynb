{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
   "metadata": {},
   "source": [
    "# Lab | Tools prompting\n",
    "\n",
    "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b94240",
   "metadata": {},
   "source": [
    "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
    "\n",
    "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide for more information.\n",
    "\n",
    "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
    "\n",
    "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
    "\n",
    "<br>\n",
    "\n",
    "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet langchain langchain-community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
   "metadata": {},
   "source": [
    "If you'd like to use LangSmith, uncomment the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
   "metadata": {},
   "source": [
    "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](https://python.langchain.com/docs/integrations/chat), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
    "```\n",
    "\n",
    "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](https://python.langchain.com/docs/integrations/chat/ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1463",
   "metadata": {},
   "source": [
    "\n",
    "#  How to Install and Run Ollama with the Phi-3 Model\n",
    "\n",
    "This guide walks you through installing **Ollama** and running the **Phi-3** model on Windows.\n",
    "\n",
    "---\n",
    "\n",
    "## Windows\n",
    "\n",
    "1. **Download Ollama for Windows**  \n",
    "   Go to: [https://ollama.com/download](https://ollama.com/download)  \n",
    "   Download and run the installer.\n",
    "\n",
    "2. **Verify Installation**  \n",
    "   Open **Command Prompt** and type:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run the Phi-3 Model**  \n",
    "   In the same terminal:\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "4. **If you get a CUDA error (GPU memory issue)**  \n",
    "   Run Ollama in **CPU mode**:\n",
    "   ```bash\n",
    "   set OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  Notes\n",
    "\n",
    "- The first time you run `ollama run phi3`, it will **download the model**, so make sure you‚Äôre connected to the internet.\n",
    "- Once downloaded, it works **offline**.\n",
    "- Keep the terminal open and running in the background while using Ollama from your code or notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946881",
   "metadata": {},
   "source": [
    "## Create a tool\n",
    "\n",
    "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](https://python.langchain.com/docs/how_to/custom_tools/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"Add two numbers.\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "tools = [multiply, add]\n",
    "\n",
    "# Let's inspect the tools\n",
    "for t in tools:\n",
    "    print(\"--\")\n",
    "    print(t.name)\n",
    "    print(t.description)\n",
    "    print(t.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be77e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"x\": 4, \"y\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
   "metadata": {},
   "source": [
    "## Creating our prompt\n",
    "\n",
    "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063b564-25ca-4729-a45f-ba4633175b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02f1dce-76e7-4ca9-9bac-5af496131fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\n",
    "The `arguments` should be a dictionary, with keys corresponding \n",
    "to the argument names and the values corresponding to the requested values.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8623e03-60eb-4439-b57b-ecbcebc61b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model\n",
    "message = chain.invoke({\"input\": \"what's 3 plus 1132\"})\n",
    "\n",
    "# Let's take a look at the output from the model\n",
    "# if the model is an LLM (not a chat model), the output will be a string.\n",
    "if isinstance(message, str):\n",
    "    print(message)\n",
    "else:  # Otherwise it's a chat model\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
   "metadata": {},
   "source": [
    "## Adding an output parser\n",
    "\n",
    "We'll use the `JsonOutputParser` for parsing our models output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "chain.invoke({\"input\": \"what's thirteen times 4\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
   "metadata": {},
   "source": [
    "üéâ Amazing! üéâ We now instructed our model on how to **request** that a tool be invoked.\n",
    "\n",
    "Now, let's create some logic to actually run the tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
   "metadata": {},
   "source": [
    "## Invoking the tool üèÉ\n",
    "\n",
    "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke \n",
    "the tool.\n",
    "\n",
    "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faee95e0-4095-4310-991f-9e9465c6738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"A function that we can use the perform a tool invocation.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: a dict that contains the keys name and arguments.\n",
    "            The name must match the name of a tool that exists.\n",
    "            The arguments are the arguments to that tool.\n",
    "        config: This is configuration information that LangChain uses that contains\n",
    "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
    "\n",
    "    Returns:\n",
    "        output from the requested tool\n",
    "    \"\"\"\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "    name = tool_call_request[\"name\"]\n",
    "    requested_tool = tool_name_to_tool[name]\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
   "metadata": {},
   "source": [
    "Let's test this out üß™!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
   "metadata": {},
   "source": [
    "## Let's put it together\n",
    "\n",
    "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0555b384-fde6-4404-86e0-7ea199003d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.83784653"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
   "metadata": {},
   "source": [
    "## Returning tool inputs\n",
    "\n",
    "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45404406-859d-4caa-8b9d-5838162c80a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply',\n",
       " 'arguments': {'x': 13, 'y': 4.14137281},\n",
       " 'output': 53.83784653}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)\n",
    ")\n",
    "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
    "\n",
    "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
    "\n",
    "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
    "\n",
    "1. Provide few shot examples.\n",
    "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df41d82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bdb22",
   "metadata": {},
   "source": [
    "# üîß Improved Tool Calling Pipeline for Mathematical Operations using LangChain\n",
    "\n",
    "This notebook implements an enhanced version of a tool-calling pipeline using LangChain, specifically designed to interface with the **Phi3** model through **Ollama**.  \n",
    "The pipeline is **more robust**, **modular**, and **error-tolerant** than the basic \"happy path\" implementation.\n",
    "\n",
    "### üéØ Key Improvements:\n",
    "- **Few-shot examples** included in the system prompt to guide the model toward correct structured outputs.\n",
    "- **Error handling and retry logic** added when parsing model outputs.\n",
    "- **Full modularization** of tools, prompt setup, parsing, and invocation into cleanly separated cells.\n",
    "- **Validation and fallback** mechanisms if the model output is incorrect.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Notebook Structure:\n",
    "\n",
    "## üì¶ Cell 1: Imports and Model Initialization\n",
    "**Purpose:**  \n",
    "- Install and import all required libraries.\n",
    "- Initialize basic components like the model and prompt templates.\n",
    "- Fix warnings related to invalid package installations if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Cell 2: Tool Definitions\n",
    "**Purpose:**  \n",
    "- Define mathematical tools (`imp_multiply`, `imp_add`) using `@tool` decorator.\n",
    "- Organize all available tools into the `imp_tools` list.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úèÔ∏è Cell 3: Prompt Setup with Few-Shot Examples\n",
    "**Purpose:**  \n",
    "- Create a robust system prompt by:\n",
    "  - Listing available tools.\n",
    "  - Providing **few-shot** examples showing correct JSON output format.\n",
    "- Set up the `imp_prompt` using `ChatPromptTemplate` from LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Cell 4: Tool Invocation with Error Handling\n",
    "**Purpose:**  \n",
    "- Create a function `imp_invoke_tool` that:\n",
    "  - Maps tool names to their corresponding callable tools.\n",
    "  - Invokes the correct tool based on parsed model output.\n",
    "  - Handles any invocation errors gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Cell 5: Chain Creation with Retry Logic\n",
    "**Purpose:**  \n",
    "- Build an end-to-end chain:\n",
    "  - Runs the prompt through the model.\n",
    "  - Parses the output.\n",
    "  - Validates the response structure.\n",
    "  - If the output is invalid, retries up to 3 attempts before returning an error.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Cell 6: Test the Improved Chain\n",
    "**Purpose:**  \n",
    "- Test the complete improved pipeline with:\n",
    "  - A correct mathematical query (`thirteen times 4.14137281`).\n",
    "  - An erroneous query (`thirteen divided by 4`) to demonstrate error catching.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Summary\n",
    "The improved tool-calling pipeline ensures the model:\n",
    "- Selects appropriate tools.\n",
    "- Returns outputs in a strict JSON format.\n",
    "- Recovers gracefully from mistakes through retries and error reporting.\n",
    "\n",
    "This structure is designed to be **extendable** (more tools can be added) and **reliable** (handles bad model outputs intelligently).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b593e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Model Initialization\n",
    "%pip install --upgrade --quiet langchain langchain-community langchain_openai langchain-ollama\n",
    "import os\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "from typing import Any, Dict, Optional, TypedDict\n",
    "from langchain_core.runnables import RunnableConfig, RunnablePassthrough\n",
    "\n",
    "# Check langchain version\n",
    "import langchain\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "\n",
    "# Initialize model\n",
    "imp_model = Ollama(model=\"phi3\") # Connects to the Phi3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd032869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Tool Definitions\n",
    "@tool\n",
    "def imp_multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "@tool\n",
    "def imp_add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "imp_tools = [imp_multiply, imp_add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e48c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Prompt Setup with Few-Shot Examples\n",
    "imp_rendered_tools = render_text_description(imp_tools)  # Generates tool descriptions\n",
    "\n",
    "# Few-shot examples with fully escaped curly braces\n",
    "imp_few_shot_examples = \"\"\"\\\n",
    "Example responses:\n",
    "1. Input: \"what's 5 times 10\"\n",
    "   Output: {{\"name\": \"multiply\", \"arguments\": {{\"x\": 5, \"y\": 10}}}}\n",
    "2. Input: \"add 7 and 3\"\n",
    "   Output: {{\"name\": \"add\", \"arguments\": {{\"x\": 7, \"y\": 3}}}}\n",
    "3. Input: \"multiply 2.5 by 4\"\n",
    "   Output: {{\"name\": \"multiply\", \"arguments\": {{\"x\": 2.5, \"y\": 4}}}}\n",
    "\"\"\"\n",
    "\n",
    "# Simplified system prompt with examples\n",
    "imp_system_prompt = f\"\"\"\\\n",
    "You are an assistant with access to the following tools:\n",
    "{imp_rendered_tools}\n",
    "\n",
    "Given the user input, select the appropriate tool and return a JSON response with two keys: \"name\" (the tool to use) and \"arguments\" (a dictionary of argument names and values). Follow the format shown in the examples:\n",
    "\n",
    "{imp_few_shot_examples}\n",
    "\"\"\"\n",
    "\n",
    "# Prompt template expecting only {input}\n",
    "imp_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", imp_system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Debug: Verify prompt and variables\n",
    "print(\"System Prompt:\\n\", imp_system_prompt)\n",
    "print(\"Prompt Template Variables:\", imp_prompt.input_variables)\n",
    "try:\n",
    "    rendered_prompt = imp_prompt.invoke({\"input\": \"test input\"})\n",
    "    print(\"Rendered Prompt:\\n\", rendered_prompt)\n",
    "except Exception as e:\n",
    "    print(\"Prompt Rendering Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55698b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Tool Invocation with Error Handling\n",
    "class ImpToolCallRequest(TypedDict):  # typing: Ensures type safety for tool call requests\n",
    "    \"\"\"A typed dict for tool call requests.\"\"\"\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "def imp_invoke_tool(\n",
    "    tool_call_request: ImpToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"Invoke a tool with error handling.\"\"\"\n",
    "    try:\n",
    "        tool_name_to_tool = {tool.name: tool for tool in imp_tools}\n",
    "        name = tool_call_request[\"name\"]\n",
    "        if name not in tool_name_to_tool:\n",
    "            raise ValueError(f\"Tool '{name}' not found.\")\n",
    "        requested_tool = tool_name_to_tool[name]\n",
    "        return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)  # langchain_core.runnables: Config passed to tool\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Tool invocation failed: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de8b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Chain Creation with Retry Logic\n",
    "# Library Explanations:\n",
    "# - langchain_core.output_parsers.JsonOutputParser: Parses model output to JSON.\n",
    "# - langchain_core.runnables.RunnablePassthrough: Assigns tool output.\n",
    "def imp_create_chain():\n",
    "    \"\"\"Create a chain with error handling and retry logic.\"\"\"\n",
    "    def handle_model_output(input_dict: dict) -> dict:\n",
    "        max_attempts = 3\n",
    "        attempt = 1\n",
    "        user_input = input_dict[\"input\"]\n",
    "        while attempt <= max_attempts:\n",
    "            try:\n",
    "                # Run the model and parse output\n",
    "                raw_output = imp_model.invoke(imp_prompt.invoke({\"input\": user_input}).to_string())\n",
    "                parsed_output = JsonOutputParser().parse(raw_output)\n",
    "                \n",
    "                # Validate output format\n",
    "                if not isinstance(parsed_output, dict) or \"name\" not in parsed_output or \"arguments\" not in parsed_output:\n",
    "                    raise ValueError(\"Invalid output format\")\n",
    "                \n",
    "                return parsed_output\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt > max_attempts:\n",
    "                    return {\"error\": f\"Failed to process input after {max_attempts} attempts: {str(e)}\"}\n",
    "                # Feed error back to model\n",
    "                user_input = f\"Previous attempt failed with error: {str(e)}. Please correct and try again: {user_input}\"\n",
    "        return {\"error\": \"Max attempts reached\"}\n",
    "\n",
    "    # Create the chain\n",
    "    imp_chain = (\n",
    "        imp_prompt\n",
    "        | imp_model\n",
    "        | JsonOutputParser()\n",
    "        | RunnablePassthrough.assign(output=imp_invoke_tool)\n",
    "    )\n",
    "    return imp_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6dc92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'imp_multiply', 'arguments': {'x': 13, 'y': 4.14137281}, 'output': 53.83784653}\n",
      "{'name': 'imp_divide', 'arguments': {'x': 13, 'y': 4}, 'output': {'error': \"Tool invocation failed: Tool 'imp_divide' not found.\"}}\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test the Improved Chain\n",
    "imp_calculator_chain = imp_create_chain()\n",
    "result = imp_calculator_chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n",
    "print(result)\n",
    "\n",
    "# Test with an erroneous input\n",
    "result_error = imp_calculator_chain.invoke({\"input\": \"what's thirteen divided by 4\"})\n",
    "print(result_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
